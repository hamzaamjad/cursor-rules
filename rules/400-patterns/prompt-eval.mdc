---
description: Prompt template creation & evaluation using a standard JSON structure.
globs: prompts/**/*.json,**/prompts/**/*.json
alwaysApply: false
---
# prompt-eval.mdc

*   **Purpose**: Systematically evaluate and improve prompts.
*   **Requirements**:
    1. All new or significantly modified LLM prompts intended for reuse or complex tasks MUST be defined using the JSON structure specified below. This includes populating all relevant keys.
    2. After creating or modifying any prompt definition file, perform a task retrospective following `.cursor/rules/task-retrospective.mdc`.
    3. When defining prompt content (e.g., within `global_constraints`, `directives[*].constraints`, `task.steps`, `output_specification.validation_rules`), explicitly reference or require adherence to other relevant project rules (e.g., `logging-monitoring.mdc`, `test-driven-development.mdc`, `python-clean-code.mdc`) where applicable.
    4. **For research-oriented prompts, include diverse examples or case studies to cover a wide range of scenarios and provide clearer guidance for users.**
    5. **Ensure that the error handling section in research-oriented prompts includes specific instructions for addressing common challenges or edge cases in the research process.**
    6. **Define specific, measurable, and achievable success criteria in the task section to facilitate effective evaluation and prioritization.**
    7. **Ensure that the examples section covers a wide range of scenarios, including edge cases and error handling, to provide comprehensive guidance and anticipate potential challenges.**
    8. **When using a comprehensive template (like the one below), clearly distinguish or prioritize essential fields based on task complexity, ensuring core elements (`prompt_metadata`, primary `objective`, `input_data`, `output_specification`) are always addressed.**
*   **Validation**:
    *   Check: Does the output include all required sections (metadata, agent configuration, context, task, examples, reasoning guidance, output specification, error handling)?
    *   Check: Is the task broken down into clear, actionable steps?
    *   Check: Are the examples relevant and illustrative of the desired output?
    *   Check: Does the reasoning guidance provide a clear approach for generating the output?
    *   Check: Are the output format, structure, and validation rules clearly specified?
    *   Check: Are potential errors and edge cases considered and handled appropriately?
    *   **Check: Does the new rule potentially overlap with or conflict with any existing rules in the .cursor/rules directory?**
    *   Check (Schema Validation): The JSON prompt definition MUST validate against the implicit schema defined by this file's structure. (A formal JSON schema could be generated from this for automated validation).
    *   Check (Completeness): All non-optional top-level keys MUST be present. Key sub-fields like `prompt_metadata.title`, `task.objective`, `output_specification.format` must be filled.
    *   Check (Code Review): When a prompt is created or modified, review the corresponding JSON definition file for clarity, completeness, and adherence to this structure.
    *   *Check (Programmatic Validation)*: For complex JSON generation tasks, consider adding a programmatic validation step (e.g., using a Python script with `jsonschema`) after generation to catch structural errors not caught by basic linting.
    *   **Check (Examples Diversity): For research-oriented prompts, ensure that the examples cover a diverse range of scenarios and provide clear guidance for users.**
    *   **Check (Error Handling Specificity): Verify that the error handling section in research-oriented prompts includes specific instructions for addressing common challenges or edge cases in the research process.**
*   **Structure Definition & Example**: The following JSON object exemplifies the required structure. New prompt definitions should replicate this structure with appropriate content.

```json
{
    "prompt_metadata": {
      "title": "Prompt Creation & Evaluation Template",
      "version": "6.0",
      "description": "Defines the standard structure for LLM prompts.",
      "last_updated": "2024-07-26", // Update this when structure changes
      "task_id": "PROMPT-EVAL-TEMPLATE",
      "compatible_models": [
        "GPT-4o",
        "Claude 3 Opus",
        "Gemini 1.5 Pro",
        "Llama 3 70B",
        "Mistral Large"
      ]
    },
    "agent_configuration": {
      "persona": {
        "role": "[Specify Role, e.g., Code Reviewer, Data Analyst]",
        "expertise_level": "[expert | intermediate | novice]",
        "tone": "[technical | formal | conversational]",
        "audience": "[Specify Audience, e.g., Developers, Product Managers]"
      },
      "global_constraints": {
        "must": [
          "[Constraint 1, e.g., Adhere strictly to output format]",
          "[Constraint 2, e.g., Cite sources for claims]"
        ],
        "must_not": [
          "[Negative Constraint 1, e.g., Reveal internal thought process]",
          "[Negative Constraint 2, e.g., Generate unsafe content]"
        ],
        "quality": [
          "[Quality Dimension 1 > Quality Dimension 2, e.g., Accuracy > Brevity]"
        ]
      },
      "directives": [
        {
          "id": "D1",
          "type": "primary",
          "objective": "[Main objective of the prompt, e.g., Generate Python unit tests]",
          "constraints": [
            "[Directive Constraint 1, e.g., Cover all public methods]",
            "[Directive Constraint 2, e.g., Use pytest framework]"
          ],
          "examples": [
            "[Illustrative example of the task]"
          ]
        }
        // Add more directives if needed (D2, D3...)
      ]
    },
    "context": {
      "background": "[Provide necessary background information for the task]",
      "supplemental_knowledge": [
        {
          "id": "K1",
          "source": "[Source of knowledge, e.g., Internal Style Guide, API Docs URL]",
          "content": "[Brief summary or key point from the source]"
        }
        // Add more knowledge sources if needed (K2, K3...)
      ],
      "input_data": {
        "format": "[Expected input format, e.g., python_code, json, text]",
        "content": "[Can be a simple string, CDATA block, or a JSON object. Prefer JSON object for structured/nested data.]"
        // Example as JSON object:
        // "content": {
        //   "placeholder": "{placeholder_for_actual_input}",
        //   "example_data": { "key": "value" }
        // }
      },
      "temporal_context": "[Specify time relevance, e.g., Current, Q3 2024]"
    },
    "task": {
      "objective": "[Specific, measurable goal for this prompt execution]",
      "success_criteria": [
        "[Criterion 1, e.g., Generated code passes linter]",
        "[Criterion 2, e.g., Output JSON validates against schema X]"
      ],
      "steps": [
        { "id": 1, "instruction": "[Step 1 instruction for the LLM]" },
        { "id": 2, "instruction": "[Step 2 instruction for the LLM]" }
        // Add more steps as needed
      ]
    },
    "examples": [
      {
        "id": "E1",
        "input": "[Example input data. Prefer structured object (JSON) over escaped strings if input is complex.]",
        "process": "[Optional: High-level description of process for this example]",
        "output": "[Example desired output corresponding to the input]"
      }
      // Add more few-shot examples if helpful (E2, E3...)
    ],
    "reasoning_guidance": {
      "approach": "[Suggested reasoning approach, e.g., Think step-by-step, Chain-of-thought]",
      "considerations": [
        "[Factor 1 to consider, e.g., Edge cases]",
        "[Factor 2 to consider, e.g., Performance implications]"
      ],
      "show_work": false, // Typically false for final output, true if debugging/transparency needed
      "evaluation_criteria": [
        "[Criterion for self-evaluation 1, e.g., Correctness]",
        "[Criterion for self-evaluation 2, e.g., Adherence to format]"
      ]
    },
    "output_specification": {
      "format": "[Expected output format, e.g., json, markdown, python_code]",
      "structure": "[Description or schema of the expected output structure]",
      "length": "[Constraint on length, e.g., <= 500 words, ~10 lines]",
      "validation_rules": [
        "[Rule 1 for validating the LLM's output, e.g., JSON must contain 'results' key]",
        "[Rule 2, e.g., Code must include type hints]"
      ],
      "excluded_elements": [
        "[Element 1 to exclude, e.g., Preamble/explanatory text]",
        "[Element 2 to exclude, e.g., Logging statements]"
      ]
    },
    "error_handling": {
      "ambiguity_resolution": {
        "protocol": "[How to handle ambiguity, e.g., Ask one clarifying question OR state assumptions.]",
        "max_iterations": 2
      },
      "missing_information": "[Action if info is missing, e.g., State what's missing; proceed best-effort or halt as instructed.]",
      "constraint_violations": {
        "action": "[Action if constraints violated, e.g., Halt and report violation.]"
      }
    }
}
```

*   **Changes**: Clarified the purpose â€“ this file *is* the structural definition and the rule is to *use* this structure. Added explicit requirement to use this JSON structure. Added schema validation and completeness checks. Simplified the understanding of the rule's role. Reframed the JSON block as the structure definition / template example itself, using placeholders for content fields.
    *   *Added note to prefer structured JSON objects over escaped strings for complex nested data in `input_data.content` and `examples[*].input` based on retrospective feedback.* 
    *   *Added a validation check recommending programmatic JSON validation for complex generation.* 
    *   Added a requirement to ensure prompt content incorporates other relevant project rules.
    *   **Added requirements and validation checks for including diverse examples and comprehensive error handling guidance in research-oriented prompts.**
    *   **Added guidance to clarify essential vs. optional fields within the prompt template based on task complexity.**
*   **Source References**: `.cursor/rules/prompt-eval.mdc`
