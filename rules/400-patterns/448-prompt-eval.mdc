---
description: "Systematic prompt evaluation framework"
version: 3.0.0
performance:
  avg_tokens: 800
  p95_latency: 1.8ms
  token_budget: 1000
dependencies:
  - "001-philosophers-stone.mdc"
lazy_load:
  - "448-prompt-eval-metrics.mdc"
dependencies:
  - 000-core/001-philosophers-stone.mdc

created: 2025-07-17
---

# Prompt Evaluation Framework

**Purpose**: Systematically assess and improve prompt effectiveness through metrics.

## Evaluation Matrix

| Dimension | Metrics | Target |
|-----------|---------|--------|
| Clarity | Ambiguity score (0-1) | <0.2 |
| Completeness | Coverage % | >90% |
| Efficiency | Token usage | <1000 |
| Accuracy | Success rate | >95% |

## Quick Evaluation Process

1. **Baseline Test**
   ```python
   results = evaluate_prompt(baseline_prompt, test_cases[:10])
   baseline_score = calculate_metrics(results)
   ```

2. **Iterative Refinement**
   ```python
   for variant in prompt_variants:
       score = evaluate_prompt(variant, test_cases)
       if score > baseline_score * 1.1:  # 10% improvement
           baseline_prompt = variant
   ```

3. **A/B Testing**
   ```yaml
   test_config:
     control: original_prompt
     variant: optimized_prompt
     sample_size: 100
     confidence: 0.95
   ```

## Key Metrics

- **Task Completion Rate**: % successful outputs
- **Token Efficiency**: Output quality / tokens used
- **Consistency Score**: Std deviation across runs
- **Error Types**: Classification of failures

## Validation Checklist
- [ ] Define success criteria upfront
- [ ] Use diverse test cases (edge cases included)
- [ ] Measure against baseline
- [ ] Statistical significance (p < 0.05)
- [ ] Document winning variants

## Common Pitfalls
1. Testing on too few examples
2. Ignoring edge cases
3. Optimizing for wrong metrics
4. Not controlling variables

## References
- Internal: `@patterns/prompt-engineering-guide.mdc`
- Tool: `scripts/prompt_evaluator.py`
