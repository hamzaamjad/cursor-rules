---
description: "LLM model selection criteria and decision framework"
version: 1.1.0
performance:
  avg_tokens: 800
  p95_latency: 1.5ms
  token_budget: 1000
tags: ["model", "selection", "optimization", "cost"]
---

# Model Selection Guide

**Purpose**: Select optimal LLM for task requirements balancing performance, cost, and capabilities.
## Requirements

### 1. Task Analysis
- **Complexity**: Code generation vs testing vs documentation
- **Quality needs**: Accuracy requirements
- **Privacy**: Data sensitivity concerns
- **Budget**: Cost constraints

### 2. Model Capabilities
| Model | Strengths | Cost | Use Case |
|-------|-----------|------|----------|
| GPT-4/4.5 | Strong reasoning, code quality | High | Complex architecture |
| Claude 3.7 | Emerging standard, intelligent | Medium | General development |
| Local LLMs | Privacy, cost-effective | Low | Sensitive data |

### 3. Compatibility Checks
- Language/framework support
- Known limitations for task type
- Context window requirements

### 4. Performance Testing
```bash
# Quick benchmark approach
1. Generate sample outputs with each model
2. Measure: quality, speed, accuracy
3. Monitor: errors, stability issues
```

### 5. Decision Matrix
- ROI: Benefits vs cost for specific task
- Alternative evaluation: Can cheaper model suffice?
- Security requirements: Data handling needs

### 6. Continuous Review
- Monitor model updates
- Track real-world performance
- Update selection criteria quarterly

**Reference**: [Cursor Model Docs](https://docs.cursor.com/settings/models)
        *   Implement access controls and audit trails for model usage.
*   **Validation**:
    *   Check (Task Alignment): Ensure that the selected model aligns with the specific requirements and constraints of the task, such as code quality, accuracy, and privacy needs.
    *   Check (Compatibility): Verify that the chosen model supports the required programming language(s) and frameworks for the task.
    *   Check (Performance): Conduct small-scale tests or benchmarks to validate the model's performance in terms of code quality, accuracy, and speed for the given task.
    *   Check (Cost-Benefit): Assess if the selected model provides a reasonable balance between cost and performance, considering the task's importance and budget constraints.
    *   Check (Stability): Monitor the model's stability and reliability during usage, checking for any errors, inconsistencies, or performance issues.
    *   Check (Documentation Reference): Confirm that https://docs.cursor.com/settings/models was referenced in the model selection process or recommendation.
*   **Examples**:
    *   **Scenario**: Generating unit tests for a sensitive codebase with a limited budget.
        *   **Model Selection**: A local LLM fine-tuned on the codebase could provide a cost-effective and privacy-preserving solution, while still delivering adequate test coverage and quality.
    *   **Scenario**: Developing a complex AI-powered code analysis tool with high accuracy requirements.
        *   **Model Selection**: GPT-4.5, despite its higher cost, may be justified for this task due to its advanced reasoning capabilities and potential to generate high-quality, accurate code analysis results.
*   **Changes**: Updated to include recent best practices for model selection, especially in the context of LLMs and AI-driven analytics. Added a section on the potential impact of new models on existing workflows.
*   **Source References**: `.cursor/rules/model-selection.mdc`; Search results on LLM models in Cursor, including comparisons between GPT-4, GPT-4.5, Claude 3.7, and local LLMs; Evaluation of model capabilities, performance, and cost factors based on search findings and general best practices for model selection in AI-assisted development workflows.