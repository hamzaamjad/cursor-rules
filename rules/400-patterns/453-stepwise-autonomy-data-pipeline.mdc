---
version: 2.0.0
lastUpdated: '2025-07-06'
author: cursor-rules-optimization
description: Data pipeline verification procedures extending stepwise-autonomy principles
category: 400-patterns
alwaysApply: false
globs:
- '**/etl/**/*.py'
- '**/pipelines/**/*.sql'
- data_processing/**/*.py
performance:
  tokenReduction: 73%
  accuracyImprovement: 0%
  processingOverhead: minimal
  empiricalValidation: Extends stepwise-autonomy verification patterns for data contexts
dependencies:
  required:
  - stepwise-autonomy
  recommended:
  - 200-domain/212-sql-correctness
  - 000-core/004-risk-checkpoint
  - 200-domain/213-sql-performance
  incompatible: []
conflicts: []
tags:
- patterns
- best-practices
- quality
- performance
- data-engineering
- verification
research: []
examples: []
notes: 'Comprehensive implementation examples available in:

  - notepads/400-patterns/stepwise-autonomy-data-pipeline/schema-validation-patterns.md

  - notepads/400-patterns/stepwise-autonomy-data-pipeline/performance-monitoring-patterns.md

  - notepads/400-patterns/stepwise-autonomy-data-pipeline/research-integration-patterns.md

  '
---


# Stepwise Autonomy Data Pipeline

## Purpose

This rule extends the stepwise-autonomy protocol with specialized verification procedures for data pipelines, ensuring reliability, performance, and correctness in data analytics workflows. The pattern emphasizes systematic validation at each pipeline stage while maintaining alignment with research findings and performance objectives.

## Core Verification Framework

Data pipeline verification follows a structured approach that validates data integrity, transformation logic, and performance characteristics at each execution stage. This framework integrates seamlessly with the base stepwise-autonomy protocol while adding data-specific verification requirements.

### Schema Validation Requirements

Schema validation forms the foundation of data pipeline reliability. Input data must undergo comprehensive validation including column presence verification, data type consistency checks, and completeness assessment against business rules. Output schema validation ensures that transformations produce expected results with appropriate data ranges and uniqueness constraints.

For detailed schema validation patterns and implementation examples, see `@Notepad:notepads/400-patterns/stepwise-autonomy-data-pipeline/schema-validation-patterns.md`.

### Transformation Logic Verification

Business logic validation requires systematic verification of aggregation accuracy, calculated field correctness, and temporal consistency. Special attention must be given to timezone handling in datetime operations, ensuring consistent treatment across all pipeline stages. Boundary conditions and edge cases require explicit testing to prevent data quality issues in production.

The verification process includes manual spot-checking of calculated values against known results, automated comparison of aggregation totals, and validation of null handling in all transformations. These verification steps integrate with the stepwise-autonomy checkpoints to ensure comprehensive quality assurance.

### Performance Monitoring Integration

Pipeline performance monitoring encompasses memory utilization tracking, query execution analysis, and end-to-end latency measurement. Each pipeline stage requires performance instrumentation to identify bottlenecks and validate resource consumption against established baselines.

Memory monitoring prevents out-of-memory conditions through proactive tracking and chunked processing strategies. Query performance analysis identifies optimization opportunities through execution plan review and index utilization verification. Comprehensive performance metrics enable trend analysis and capacity planning.

For detailed performance monitoring patterns and tools, see `@Notepad:notepads/400-patterns/stepwise-autonomy-data-pipeline/performance-monitoring-patterns.md`.

### Research Alignment Verification

Pipeline implementations must maintain explicit alignment with research findings that informed design decisions. This includes documenting the connection between implementation choices and empirical evidence, validating actual performance against research-identified metrics, and maintaining decision transparency through structured logging.

Research alignment verification ensures that production systems achieve the performance characteristics identified during experimental validation. Continuous monitoring detects drift from research baselines, triggering investigation when significant deviations occur.

For comprehensive research integration patterns, see `@Notepad:notepads/400-patterns/stepwise-autonomy-data-pipeline/research-integration-patterns.md`.

## Integration with Core Stepwise Autonomy

This specialized rule seamlessly extends the core stepwise-autonomy protocol for data pipeline contexts. When implementing data pipelines, follow the general stepwise process while incorporating these data-specific verification steps at appropriate checkpoints. Pay particular attention to input data validation early in the process and maintain intermediate state visibility for debugging complex transformations.

The data pipeline verification procedures complement rather than replace the core protocol's requirements. Task complexity assessment, resource verification, and progressive validation from the base protocol remain essential, with these specialized patterns providing additional depth for data-specific challenges.

## Validation Checklist

Comprehensive pipeline validation requires systematic verification across multiple dimensions. Schema integrity must be confirmed at pipeline boundaries. Transformation accuracy requires validation through test cases and spot checks. Performance characteristics must align with established baselines and SLAs. Research alignment ensures that implementation decisions remain grounded in empirical evidence.

Each validation checkpoint produces explicit pass/fail indicators that guide pipeline execution decisions. Failed validations trigger appropriate remediation strategies based on criticality and business impact. The validation framework supports both fail-fast and warning-based approaches, allowing pipelines to adapt their strictness based on data criticality and business requirements.