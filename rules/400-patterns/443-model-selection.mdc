---
description: "LLM model selection criteria for Cursor tasks"
version: 2.0.0
performance:
  token_budget: 600
  validation_target: 2ms
references:
  - https://docs.cursor.com/settings/models
dependencies:
  - 000-core/001-philosophers-stone.mdc

created: 2025-07-17
---

# Model Selection

**Purpose**: Select optimal LLM model based on task requirements and constraints.

## Decision Matrix

| Task Type | Model | Rationale |
|-----------|-------|-----------|
| Complex reasoning | GPT-4.5 | Advanced capabilities justify cost |
| Code generation | Claude 3.7 | Best code performance/cost ratio |
| Sensitive data | Local LLM | Privacy, on-device processing |
| Unit tests | GPT-4 | Balance of quality/cost |
| Documentation | Claude 3.7 | Natural language excellence |

## Selection Criteria

### Task Requirements
- Complexity level (simple → complex)
- Code quality needs (prototype → production)
- Privacy constraints (public → sensitive)
- Budget limits ($/request thresholds)

### Model Capabilities
```yaml
GPT-4.5:
  strengths: [reasoning, complex_tasks]
  cost: high
  context: 128k
  
Claude-3.7:
  strengths: [code_quality, stability]
  cost: medium
  context: 200k
  
Local-LLMs:
  strengths: [privacy, cost_efficiency]
  cost: compute_only
  context: varies
```

### Quick Tests
1. Generate sample output with 2-3 models
2. Compare: accuracy, speed, cost
3. Monitor: errors, stability issues

## Validation
- Model supports required languages ✓
- Cost justified by task value ✓
- Performance meets requirements ✓
- Privacy needs satisfied ✓

## Examples
```python
# High-stakes production code
model = "gpt-4.5"  # Maximum accuracy

# Routine test generation  
model = "claude-3.7"  # Good balance

# Sensitive financial data
model = "local-llama-70b"  # On-premise only
```