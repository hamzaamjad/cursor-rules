---
version: 1.0.0
lastUpdated: '2025-07-06'
author: migrated
description: null
category: 400-patterns
alwaysApply: true
globs: null
performance:
  tokenReduction: 0%
  accuracyImprovement: 0%
  processingOverhead: minimal
  empiricalValidation: To be measured
dependencies:
  required: []
  recommended:
  - risk-checkpoint
  incompatible: []
conflicts: []
tags:
- patterns
- best-practices
- quality
- performance
research: []
examples: []
notes: ''
---

# model-selection.mdc

*   **Purpose**: To provide guidance on selecting the most appropriate LLM model for a given prompt or task in Cursor, considering factors such as model capabilities, performance, cost, and compatibility.
*   **Requirements**:
    1.  Evaluate the specific requirements and constraints of the task, such as:
        *   Complexity and scope of the task (e.g., code generation, testing, documentation)
        *   Required level of code quality and accuracy
        *   Sensitivity of the codebase and data privacy concerns
        *   Budget and cost considerations
    2.  Consider the capabilities and strengths of available models:
        *   GPT-4 and GPT-4.5: Powerful models with strong code generation and reasoning abilities, but higher cost per request
        *   Claude 3.7: Emerging as a potential standard model in Cursor due to its intelligence and code performance, but may have initial stability issues
        *   Local LLMs: Offer privacy, data security, and cost-effectiveness, but may require more setup and fine-tuning
    3.  Assess the compatibility of the model with the specific task and programming language:
        *   Verify that the model supports the required programming language(s) and frameworks
        *   Check for any known limitations or issues with the model for the given task type
    4.  Conduct small-scale tests or benchmarks to compare model performance:
        *   Generate sample code or tests using different models
        *   Evaluate the quality, accuracy, and speed of the generated output
        *   Monitor for any errors, inconsistencies, or stability issues
    5.  Consider the trade-offs between cost and performance:
        *   Determine if the benefits of a higher-cost model (e.g., GPT-4.5) justify the expense for the specific task
        *   Evaluate if a lower-cost or open-source alternative (e.g., local LLM) can sufficiently meet the requirements
    6.  Continuously monitor and reassess model selection as new models and capabilities become available:
        *   Stay informed about updates and improvements to existing models
        *   Evaluate new models as they are introduced in Cursor
        *   Regularly review and update the model selection criteria based on real-world performance and evolving needs
    7.  Always reference the official Cursor model documentation (https://docs.cursor.com/settings/models) when making or recommending model selection decisions. This ensures up-to-date awareness of available models, context window sizes, pricing, and agentic capabilities.
    8.  **Security Considerations**:
        *   Ensure data privacy and security are prioritized when selecting models, especially for sensitive tasks.
        *   Implement access controls and audit trails for model usage.
*   **Validation**:
    *   Check (Task Alignment): Ensure that the selected model aligns with the specific requirements and constraints of the task, such as code quality, accuracy, and privacy needs.
    *   Check (Compatibility): Verify that the chosen model supports the required programming language(s) and frameworks for the task.
    *   Check (Performance): Conduct small-scale tests or benchmarks to validate the model's performance in terms of code quality, accuracy, and speed for the given task.
    *   Check (Cost-Benefit): Assess if the selected model provides a reasonable balance between cost and performance, considering the task's importance and budget constraints.
    *   Check (Stability): Monitor the model's stability and reliability during usage, checking for any errors, inconsistencies, or performance issues.
    *   Check (Documentation Reference): Confirm that https://docs.cursor.com/settings/models was referenced in the model selection process or recommendation.
*   **Examples**:
    *   **Scenario**: Generating unit tests for a sensitive codebase with a limited budget.
        *   **Model Selection**: A local LLM fine-tuned on the codebase could provide a cost-effective and privacy-preserving solution, while still delivering adequate test coverage and quality.
    *   **Scenario**: Developing a complex AI-powered code analysis tool with high accuracy requirements.
        *   **Model Selection**: GPT-4.5, despite its higher cost, may be justified for this task due to its advanced reasoning capabilities and potential to generate high-quality, accurate code analysis results.
*   **Changes**: Updated to include recent best practices for model selection, especially in the context of LLMs and AI-driven analytics. Added a section on the potential impact of new models on existing workflows.
*   **Source References**: `.cursor/rules/model-selection.mdc`; Search results on LLM models in Cursor, including comparisons between GPT-4, GPT-4.5, Claude 3.7, and local LLMs; Evaluation of model capabilities, performance, and cost factors based on search findings and general best practices for model selection in AI-assisted development workflows.
