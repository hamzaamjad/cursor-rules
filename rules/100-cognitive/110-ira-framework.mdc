---
alwaysApply: true
dependencies:
  - 000-core/001-philosophers-stone.mdc

created: 2025-07-17
version: 1.0.0
---
# IRA Framework: Investigate, Research, Act

<!-- Version: 1.0.0 — 2025-07-14 -->
<!-- Based on: User Experience Research and Decision Sciences -->

* **Purpose**: Implement a systematic evidence-based approach to problem-solving that reduces ambiguity, integrates conflicting data, and ensures actionable outcomes through iterative validation.

* **Requirements**:
  * **Investigation Phase**:
    - Clearly articulate the decision trigger with measurable success criteria
    - Conduct comprehensive evidence audit of existing data and knowledge
    - Identify and document all evidence gaps systematically
    - Reframe vague problems into specific, researchable questions
  * **Research Phase**:
    - Match research methods to specific evidence gaps (generative vs evaluative)
    - Deploy mixed methods for comprehensive understanding
    - Synthesize findings using structured analysis (affinity mapping, thematic analysis)
    - Prioritize insights by impact and feasibility metrics
  * **Action Phase**:
    - Co-create solutions with relevant stakeholders
    - Implement controlled pilots with clear boundaries
    - Embed measurement protocols from the start
    - Conduct structured retrospectives for continuous improvement

* **Validation**:
  * Check: Decision trigger is documented with clear stakeholder mapping
  * Check: Evidence gaps are explicitly identified before research begins
  * Check: Research methods directly address identified gaps
  * Check: Actions include embedded evaluation mechanisms
  * Metric: Time from investigation to actionable insight < 2 weeks

* **Examples**:
  <example_correct>
  Description: Investigating API performance degradation
  ```python
  # INVESTIGATE: Define the problem space
  decision_trigger = {
      "issue": "API response time increased 40% last week",
      "stakeholders": ["engineering", "customer_success", "product"],
      "success_metrics": {"p95_latency": "<200ms", "error_rate": "<0.1%"}
  }
  
  evidence_audit = {
      "available": ["APM metrics", "error logs", "deployment history"],
      "gaps": ["user impact data", "root cause analysis", "comparative benchmarks"]
  }
  
  # RESEARCH: Fill evidence gaps with targeted methods
  research_plan = {
      "user_impact": "Query customer support tickets + session replays",
      "root_cause": "Distributed tracing + code profiling",
      "benchmarks": "Load testing with previous versions"
  }
  
  # ACT: Implement with measurement
  pilot_rollout = {
      "solution": "Implement caching layer",
      "test_group": "10% of traffic",
      "metrics": ["latency", "cache_hit_rate", "error_rate"],
      "rollback_criteria": "error_rate > 0.5%"
  }
  ```
  </example_correct>
  
  <example_incorrect>
  Description: Jumping to solutions without investigation
  ```python
  # BAD: No evidence audit or gap analysis
  def fix_performance():
      # Assumption-based solution
      add_more_servers()  # No investigation of actual bottleneck
      
      # No research phase
      deploy_to_production()  # No pilot or measurement
      
      # No reflection mechanism
      close_ticket()  # No learning captured
  ```
  </example_incorrect>

* **Integration Notes**:
  * Works best with: OODA loop for rapid iteration cycles
  * May conflict with: Pure brainstorming approaches that skip evidence gathering
  * Performance impact: Adds ~1-2 days upfront but reduces rework by 60%
  
* **Implementation Checklist**:
  * [ ] Create decision proposal template with stakeholder matrix
  * [ ] Set up evidence repository with gap tracking
  * [ ] Define research method selection criteria
  * [ ] Establish pilot success/failure thresholds
  * [ ] Schedule retrospective before starting next cycle

* **References**:
  * [User Interviews Decision-Driven Research](https://www.userinterviews.com/blog/a-framework-for-decision-driven-research)
  * [Evidence-Based Decision Making in Healthcare](https://pmc.ncbi.nlm.nih.gov/articles/PMC7953669/)
  * Internal: `@patterns/research-synthesis-methods.mdc`

## Performance Targets (New)
* **Loop Velocity Benchmarks**:
  - Investigation-to-Action cycle ≤ 14 days (p95)
  - Evidence gap closure rate ≥ 80% within single IRA cycle
* **Knowledge Capture Metrics**:
  - *Tacit-to-Explicit Ratio (TER)*: Document ≥ 60% of tacit insights as testable hypotheses per cycle
  - *Evidence Provenance Index (EPI)*: ≥ 0.9 (all evidence linked to origin + timestamp)
* **Dynamic KPI Weighting**:
  - Recalculate KPI weights every 10 OODA cycles or upon significant environmental shift (>2σ deviation in key metrics)
  - Use weighted-moving average with half-life of 30 days to adjust importance scores

## Orientation Bias Safeguards (New)
* Establish **Red-Team Review Thread** for each major investigation to surface confirmation bias
* Run **Data-Drift Detection** (KS-test p<0.05) on critical datasets before synthesis

## Compliance & Ethics Hooks (New)
* Generate **Evidence Transparency Dossier** aligning with EU AI Act Article 13 for each decision package
* Verify no personally identifiable information (PII) leakage; automated scan compliance threshold: <0.1% PII tokens